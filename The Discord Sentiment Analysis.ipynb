{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc05ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.269, 'neu': 0.598, 'pos': 0.133, 'compound': -0.9996}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"ucsb ucla strike.csv\")\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg = []\n",
    "neu = []\n",
    "pos = []\n",
    "compound = []\n",
    "sample_size = []\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b69798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.217, 'neu': 0.628, 'pos': 0.155, 'compound': 0.631}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"ucsb ucla strike.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1107_1113 = data[(data['Date']>=\"2022-11-07\") & (data['Date']<\"2022-11-13\")]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1107_1113[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1107_1113['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8170fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.249, 'neu': 0.61, 'pos': 0.141, 'compound': -0.9912}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"ucsb ucla strike.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1114_1120 = data[(data['Date']>=\"2022-11-14\") & (data['Date']<\"2022-11-20\")]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1114_1120[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1114_1120['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c33769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.203, 'neu': 0.672, 'pos': 0.125, 'compound': -0.6918}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"ucsb ucla strike.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1121_1127 = data[(data['Date']>=\"2022-11-21\") & (data['Date']<\"2022-11-27\")]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1121_1127[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1121_1127['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35e2283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.164, 'neu': 0.706, 'pos': 0.13, 'compound': -0.7297}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"ucsb ucla strike.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1128_1204 = data[(data['Date']>=\"2022-11-28\") & (data['Date']<\"2022-12-04\")]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1128_1204[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1128_1204['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c2d3f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.365, 'neu': 0.533, 'pos': 0.101, 'compound': -0.979}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"ucsb ucla strike.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1205_1211 = data[(data['Date']>=\"2022-12-05\") & (data['Date']<\"2022-12-11\")]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1205_1211[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1205_1211['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e049de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.297, 'neu': 0.567, 'pos': 0.136, 'compound': -0.9739}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"ucsb ucla strike.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1212_1218 = data[(data['Date']>=\"2022-12-12\") & (data['Date']<\"2022-12-18\")]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1212_1218[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1212_1218['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16d7a5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yeahs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.17, 'neu': 0.83, 'pos': 0.0, 'compound': -0.7827}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "data = pd.read_csv(\"ucsb ucla strike.csv\")\n",
    "data['Date'] =  pd.to_datetime(data['Date'], format='%m/%d/%y')\n",
    "data_1219_1225 = data[(data['Date']>=\"2022-12-19\") & (data['Date']<\"2022-12-25\")]\n",
    "stop_words = open(\"buckley-salton.txt\").read()\n",
    "# clean tweets\n",
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import random                              # pseudo-random number generator\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "# turn discord dataset column to string\n",
    "discord_column = data_1219_1225[['Content']]\n",
    "\n",
    "discord_string = ''\n",
    "for i in range(len(discord_column)):\n",
    "    discord_string += (str(discord_column.iloc[i])[5:])[:-13]\n",
    "    \n",
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "# tokenize discord\n",
    "discord_tokens = nltk.word_tokenize(discord_string)\n",
    "discord_clean = []\n",
    "\n",
    "for word in discord_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stop_words and  # remove stopwords\n",
    "        word not in string.punctuation and \n",
    "        word != 'Name' \n",
    "        and word.isdigit() == False \n",
    "        and word != '...'):  # remove punctuation\n",
    "        discord_clean.append(word)\n",
    "\n",
    "#removed stop words and punctuation\n",
    "discord_clean_string = \" \".join(discord_clean)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "neg.append((sia.polarity_scores(discord_clean_string))['neg'])\n",
    "neu.append(sia.polarity_scores(discord_clean_string)['neu'])\n",
    "pos.append(sia.polarity_scores(discord_clean_string)['pos'])\n",
    "compound.append(sia.polarity_scores(discord_clean_string)['compound'])\n",
    "sample_size.append(len(data_1219_1225['Content']))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(discord_clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "054829eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg: [0.217, 0.249, 0.203, 0.164, 0.365, 0.297, 0.17]\n",
      "Neu: [0.628, 0.61, 0.672, 0.706, 0.533, 0.567, 0.83]\n",
      "Pos: [0.155, 0.141, 0.125, 0.13, 0.101, 0.136, 0.0]\n",
      "Compound: [0.631, -0.9912, -0.6918, -0.7297, -0.979, -0.9739, -0.7827]\n",
      "Sample Size: [64, 112, 57, 73, 38, 45, 18]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Neg: {neg}\\nNeu: {neu}\\nPos: {pos}\\nCompound: {compound}\\nSample Size: {sample_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c31cf25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
